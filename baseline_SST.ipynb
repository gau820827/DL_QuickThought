{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import re\n",
    "import random\n",
    "\n",
    "sst_home = '../data/trees'\n",
    "\n",
    "easy_label_map = {0:0, 1:1, 2:2, 3:3, 4:4}\n",
    "\n",
    "PADDING = \"<PAD>\"\n",
    "UNKNOWN = \"<UNK>\"\n",
    "max_seq_length = 20\n",
    "\n",
    "def load_sst_data(path):\n",
    "    data = []\n",
    "    with open(path) as f:\n",
    "        for i, line in enumerate(f): \n",
    "            example = {}\n",
    "            example['label'] = easy_label_map[int(line[1])]\n",
    "            if example['label'] is None:\n",
    "                continue\n",
    "            \n",
    "            # Strip out the parse information and the phrase labels---we don't need those here\n",
    "            text = re.sub(r'\\s*(\\(\\d)|(\\))\\s*', '', line)\n",
    "            example['text'] = text[1:]\n",
    "            data.append(example)\n",
    "\n",
    "    random.seed(1)\n",
    "    random.shuffle(data)\n",
    "    return data\n",
    "     \n",
    "training_set = load_sst_data(sst_home + '/train.txt')\n",
    "dev_set = load_sst_data(sst_home + '/dev.txt')\n",
    "test_set = load_sst_data(sst_home + '/test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "def tokenize(string):\n",
    "    return string.split()\n",
    "\n",
    "def build_dictionary(training_datasets):\n",
    "    \"\"\"\n",
    "    Extract vocabulary and build dictionary.\n",
    "    \"\"\"  \n",
    "    word_counter = collections.Counter()\n",
    "    for i, dataset in enumerate(training_datasets):\n",
    "        for example in dataset:\n",
    "            word_counter.update(tokenize(example['text']))\n",
    "        \n",
    "    vocabulary = set([word for word in word_counter])\n",
    "    vocabulary = list(vocabulary)\n",
    "    vocabulary = [PADDING, UNKNOWN] + vocabulary\n",
    "        \n",
    "    word_indices = dict(zip(vocabulary, range(len(vocabulary))))\n",
    "\n",
    "    return word_indices, len(vocabulary)\n",
    "\n",
    "def sentences_to_padded_index_sequences(word_indices, datasets):\n",
    "    \"\"\"\n",
    "    Annotate datasets with feature vectors. Adding right-sided padding. \n",
    "    \"\"\"\n",
    "    for i, dataset in enumerate(datasets):\n",
    "        for example in dataset:\n",
    "            example['text_index_sequence'] = torch.zeros(max_seq_length)\n",
    "\n",
    "            token_sequence = tokenize(example['text'])\n",
    "            padding = max_seq_length - len(token_sequence)\n",
    "\n",
    "            for i in range(max_seq_length):\n",
    "                if i >= len(token_sequence):\n",
    "                    index = word_indices[PADDING]\n",
    "                    pass\n",
    "                else:\n",
    "                    if token_sequence[i] in word_indices:\n",
    "                        index = word_indices[token_sequence[i]]\n",
    "                    else:\n",
    "                        index = word_indices[UNKNOWN]\n",
    "                example['text_index_sequence'][i] = index\n",
    "\n",
    "            example['text_index_sequence'] = example['text_index_sequence'].long().view(1,-1)\n",
    "            example['label'] = torch.LongTensor([example['label']])\n",
    "\n",
    "\n",
    "word_to_ix, vocab_size = build_dictionary([training_set])\n",
    "sentences_to_padded_index_sequences(word_to_ix, [training_set, dev_set, test_set])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is the iterator we'll use during training. \n",
    "# It's a generator that gives you one batch at a time.\n",
    "def data_iter(source, batch_size):\n",
    "    dataset_size = len(source)\n",
    "    start = -1 * batch_size\n",
    "    order = list(range(dataset_size))\n",
    "    random.shuffle(order)\n",
    "\n",
    "    while True:\n",
    "        start += batch_size\n",
    "        if start > dataset_size - batch_size:\n",
    "            # Start another epoch.\n",
    "            start = 0\n",
    "            random.shuffle(order)   \n",
    "        batch_indices = order[start:start + batch_size]\n",
    "        yield [source[index] for index in batch_indices]\n",
    "\n",
    "# This is the iterator we use when we're evaluating our model. \n",
    "# It gives a list of batches that you can then iterate through.\n",
    "def eval_iter(source, batch_size):\n",
    "    batches = []\n",
    "    dataset_size = len(source)\n",
    "    start = -1 * batch_size\n",
    "    order = list(range(dataset_size))\n",
    "    random.shuffle(order)\n",
    "\n",
    "    while start < dataset_size - batch_size:\n",
    "        start += batch_size\n",
    "        batch_indices = order[start:start + batch_size]\n",
    "        batch = [source[index] for index in batch_indices]\n",
    "        batches.append(batch)\n",
    "        \n",
    "    return batches\n",
    "\n",
    "# The following function gives batches of vectors and labels, \n",
    "# these are the inputs to your model and loss function\n",
    "def get_batch(batch):\n",
    "    vectors = []\n",
    "    labels = []\n",
    "    for dict in batch:\n",
    "        vectors.append(dict[\"text_index_sequence\"])\n",
    "        labels.append(dict[\"label\"])\n",
    "    return vectors, labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(model, data_iter):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i in range(len(data_iter)):\n",
    "        vectors, labels = get_batch(data_iter[i])\n",
    "        vectors = Variable(torch.stack(vectors).squeeze())\n",
    "        labels = torch.stack(labels).squeeze()\n",
    "        output = model(vectors)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum()\n",
    "    return correct / float(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A Multi-Layer Perceptron (MLP)\n",
    "class MLPClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, embedding_dim, hidden_dim, num_labels):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        \n",
    "        self.embed = nn.Embedding(input_size, embedding_dim, padding_idx=0)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "            \n",
    "        self.linear_1 = nn.Linear(embedding_dim, hidden_dim) \n",
    "        self.linear_2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.linear_3 = nn.Linear(hidden_dim, num_labels)\n",
    "        self.init_weights()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.embed(x)\n",
    "        out = self.dropout(out)\n",
    "        out = torch.sum(out, dim=1)\n",
    "        out = F.relu(self.linear_1(out))\n",
    "        out = F.relu(self.linear_2(out))\n",
    "        out = self.dropout(self.linear_3(out))\n",
    "        return F.log_softmax(out)\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        lin_layers = [self.linear_1, self.linear_2]\n",
    "        em_layer = [self.embed]\n",
    "     \n",
    "        for layer in lin_layers+em_layer:\n",
    "            layer.weight.data.uniform_(-initrange, initrange)\n",
    "            if layer in lin_layers:\n",
    "                layer.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def training_loop(model, loss, optimizer, training_iter, dev_iter, train_eval_iter):\n",
    "    step = 0\n",
    "    for i in range(num_train_steps):\n",
    "        model.train()\n",
    "        vectors, labels = get_batch(next(training_iter))\n",
    "        vectors = Variable(torch.stack(vectors).squeeze())\n",
    "        labels = Variable(torch.stack(labels).squeeze())\n",
    "\n",
    "        model.zero_grad()\n",
    "        output = model(vectors)\n",
    "\n",
    "        lossy = loss(output, labels)\n",
    "        lossy.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            print( \"Step %i; Loss %f; Train acc: %f; Dev acc %f\" \n",
    "                %(step, lossy.data[0], evaluate(model, train_eval_iter), evaluate(model, dev_iter)))\n",
    "\n",
    "        step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyper Parameters \n",
    "input_size = vocab_size\n",
    "num_labels = 2\n",
    "hidden_dim = 50\n",
    "embedding_dim = 100\n",
    "batch_size = 32\n",
    "learning_rate = 0.004\n",
    "num_train_steps = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0; Loss 0.665356; Train acc: 0.534000; Dev acc 0.540000\n",
      "Step 100; Loss 0.717217; Train acc: 0.736000; Dev acc 0.718000\n",
      "Step 200; Loss 0.520574; Train acc: 0.866000; Dev acc 0.754000\n",
      "Step 300; Loss 0.399714; Train acc: 0.896000; Dev acc 0.758000\n",
      "Step 400; Loss 0.574587; Train acc: 0.878000; Dev acc 0.766000\n",
      "Step 500; Loss 0.339262; Train acc: 0.930000; Dev acc 0.792000\n",
      "Step 600; Loss 0.302633; Train acc: 0.948000; Dev acc 0.764000\n",
      "Step 700; Loss 0.218039; Train acc: 0.958000; Dev acc 0.790000\n",
      "Step 800; Loss 0.404281; Train acc: 0.964000; Dev acc 0.788000\n",
      "Step 900; Loss 0.148418; Train acc: 0.968000; Dev acc 0.782000\n"
     ]
    }
   ],
   "source": [
    "model = MLPClassifier(input_size, embedding_dim, hidden_dim, num_labels)\n",
    "    \n",
    "# Loss and Optimizer\n",
    "loss = nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "training_iter = data_iter(training_set, batch_size)\n",
    "train_eval_iter = eval_iter(training_set[0:500], batch_size)\n",
    "dev_iter = eval_iter(dev_set[0:500], batch_size)\n",
    "training_loop(model, loss, optimizer, training_iter, dev_iter, train_eval_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test data: 76.990664\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "test_iter = eval_iter(test_set, batch_size)\n",
    "test_acc = evaluate(model, test_iter)\n",
    "print('Accuracy of the network on the test data: %f' % (100 * test_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
